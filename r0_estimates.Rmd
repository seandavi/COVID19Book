
# The Replication rate, $R_0$

```{r include=FALSE}
library(sars2pack)
library(BiocStyle)
library(knitr)
prod=FALSE
if(Sys.getenv('ENVIRONMENT','DEV')=="PROD") {
    prod=TRUE
}
knitr::opts_chunk$set(warning=!prod, message=!prod)
```

## Learning goals and objectives

- Gain an intuitive understanding of $R_0$.
- Know that the value of $R_0$ determines how quickly a disease spreads or is eliminated.
- Name the three main drivers of $R_0$
- Learn to estimate $R_0$ from data on the number of infections over time occurring in a population.

## Background

The replication rate, $R_0$ is a central value in understanding the rate at which a disease is spreading in a susceptible population. 


### What is $R_0$?

$R_0$ is pronounced “R naught.” The $R_0$ value is an estimate of the average number of people who will be infected by one contagious person. It specifically applies to a population of people who are susceptible to the disease (have not been vaccinated and are not immune). If a disease has an $R_0$ of 18, for example, a contagious person will transmit it to an average of 18 other people, assuming that all people in the community are susceptible.

### What do $R_0$ values mean?

The $R_0$ value of a disease is important to understanding the dynamics of disease spread. Depending on the $R_0$ value, a disease should follow one of three possible courses in the at-risk community.

- If $R_0$ is less than 1, each existing infection is spread on average to less than one additional person, leading to decline in the number of cases and eventual end to the spread.
- If $R_0$ equals 1, each existing infection causes one new infection, leading to stable infection numbers without increase or decrease with time, on average.
- If $R_0$ is more than 1, each existing infection leads to more than one infection, resulting in growth and potential for epidemic/pandemic conditions.

_Importantly, the disease-specific $R_0$ value pplies when each member of the community is fully vulnerable to the disease with_:

- no one vaccinated
- no one immune
- no way to control the spread of the disease

### What variables contribute to $R_0$?

Three main factors impact the $R_0$ value of a disease:

- *Infectious period*: The time that an infected person can spread the disease varies from one disease to another. Additional factors such as age of the infected person may affect the period during which a person can infect others. A long period of infectiousness will contribute to a higher $R_0$ value.

- *Contact rate*: If a person who’s infected with a contagious disease comes into contact with many people who aren’t infected or vaccinated, the disease will spread more quickly. If that person remains at home, in a hospital, or otherwise quarantined while they’re contagious, the disease will spread more slowly. A high contact rate will contribute to a higher $R_0$ value. The corollary, that lower contact rate, can reduce $R_0$ is the basis for [flattening the curve](https://www.nytimes.com/article/flatten-curve-coronavirus.html) through [social distancing](https://en.wikipedia.org/wiki/Social_distancing).

```{r gifcontrol, fig.cap="Toby Morris (Spinoff.co.nz) / CC BY-SA (https://creativecommons.org/licenses/by-sa/4.0)",echo=FALSE, eval=FALSE}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/4/45/Covid-19-Transmission-graphic-01.gif/512px-Covid-19-Transmission-graphic-01.gif")
```


- *Mode of transmission*:
Airborne illnesses tend to have a higher $R_0$ value than those spread through contact or through bodily fluids. 


## Real data examples

```{r}
library(dplyr)
library(magrittr)
```

The software package `r CRANpkg('EpiEstim')` provides multiple methods
for estimating the time-varying reproduction number from epidemic 
curves. As a reminder, an epidemic curve typically consists of 
a set of either *cumulative* or *new* cases per unit time. In *sars2pack*,
we are careful to import all datasets as *cumulative* counts. 

### State of Maryland in the United States

As a starter, we'll work with my home state, Maryland and examine
its COVID-19 disease spread over time and then estimate the $R_0$ over 
time to see how successful Marylanders have been with social distancing.

```{r}
nyt = nytimes_state_data()
head(nyt)
```

To contrast the way cumulative vs "incidence" data look, we can look
at one state cumulatively and the same state in terms of new cases (or 
"incidence". 

Here, we isolate the state of Maryland and pull out the cumulative cases.

```{r}
md_cumulative = nyt %>%
    dplyr::filter(state=='Maryland' & subset=='confirmed' & count>50)
```

Data from most online sources and all the epidemic curve data
for which we provide accessors are *cumulative*. We can add a new 
column to the dataset using `add_incidence_column()`.

```{r}
md_full = md_cumulative %>% add_incidence_column()
head(md_full)
```

Now, plot both using the `plot_epicurve()` helper function. Note that
`plot_epicurve()` returns a ggplot object that can be further manipulated.

```{r abc, out.width='100%', fig.cap = "Epidemic curve plots (epicurves) for Maryland. Cumulative cases (A, C) and daily incidence (B, D). Top row (A, B) is linear scale on the y-axis; bottom row (C, D) with y-axis in log scale"}
library(cowplot)
pcumulative = plot_epicurve(md_full,log=FALSE)
pincidence  = plot_epicurve(md_full,case_column='inc',log=FALSE)
pcumulative_log = plot_epicurve(md_full,log=TRUE)
pincidence_log  = plot_epicurve(md_full,case_column='inc',log=TRUE)
print(cowplot::plot_grid(pcumulative,pincidence,
                   pcumulative_log,pincidence_log,ncol=2,
                   labels=c("A","B","C","D")))
```

The `estimate_Rt()` is a very lightweight wrapper around the 
`EpiEstim::estimate_R()` function. For more details on the help,
take a look at the `r CRANpkg('EpiEstim')` documentation.

We are able to use exponential growth and time-dependent models
with this data, using generation time model from a
recent [Annals of Internal Medicine](https://annals.org/aim/fullarticle/2762808/incubation-period-coronavirus-disease-2019-covid-19-from-publicly-reported) paper.


```{r}
x = estimate_Rt(md_full,method = 'parametric_si',
                config = list(mean_si=3.96, std_si=4.75))
```

Since we are looking for $R_0$ to be less than 1 for the pandemic
to be dying down, we can look at $R_0$ estimates over time.

```{r mdEpiestim,out.width="100%", fig.cap="Estimate of R_0 over time in Maryland. The grey coloring represents 95% confidence intervals around the estimate of R_0."}
library(ggplot2)
ggplot(x,aes(x=date_end,y=`Mean(R)`)) +
    geom_ribbon(aes(ymin=`Quantile.0.025(R)`,ymax=`Quantile.0.975(R)`),fill='grey75') +
    geom_line() + ggtitle('Maryland')
```

### State of Arizona in the United States

Arizona shows a somewhat different pattern of pandemic control than Maryland.
Start by creating a full epicurve dataset for Arizona.

```{r}
az_full = nyt %>%
    dplyr::filter(state=='Arizona' & subset=='confirmed' & count>25) %>%
    add_incidence_column() # defaults suffice here
```

Taking a look at the actual counts of cases and incidence can give a 
quick sense of what to expect with regard to $R_0$ over time. Note that
the *absolute number* of cases is quite different from Maryland.

```{r azEpicurve, out.width='100%', fig.cap = "Epidemic curve plots (epicurves) for Arizona. Cumulative cases (A, C) and daily incidence (B, D). Top row (A, B) is linear scale on the y-axis; bottom row (C, D) with y-axis in log scale"}
library(cowplot)
pcumulative = plot_epicurve(az_full,log=FALSE)
pincidence  = plot_epicurve(az_full,case_column='inc',log=FALSE)
pcumulative_log = plot_epicurve(az_full,log=TRUE)
pincidence_log  = plot_epicurve(az_full,case_column='inc',log=TRUE)
cowplot::plot_grid(pcumulative,pincidence,
                   pcumulative_log,pincidence_log,ncol=2,
                   labels=c("A","B","C","D"))
```

Arizona shows a resurgence in infections as evidenced by $R_0$ rising well above 1
at the beginning of June. 

```{r azEpiestim,out.width="100%", fig.cap="Estimate of R_0 over time for Arizona. The grey coloring represents 95% confidence intervals around the estimate of R_0."}
library(ggplot2)
x = estimate_Rt(az_full,method = 'parametric_si',
                config = list(mean_si=3.96, std_si=4.75))
ggplot(x,aes(x=date_end,y=`Mean(R)`)) +
    geom_ribbon(aes(ymin=`Quantile.0.025(R)`,ymax=`Quantile.0.975(R)`),fill='grey75') +
    geom_line() + ggtitle('Arizona')
```

### Brazil

```{r}
jhu = jhu_data()
head(jhu)
```

```{r}
jhu_brazil = jhu %>%
    dplyr::filter(CountryRegion=='Brazil' & subset=='confirmed' & count>50) %>%
    add_incidence_column()
```

```{r}
plot_epicurve(jhu_brazil,case_column='inc') + ggtitle('Brazil', subtitle = 'Daily new cases')
```



```{r}
x = estimate_Rt(jhu_brazil,method = 'parametric_si',
                config = list(mean_si=3.96, std_si=4.75))
ggplot(x,aes(x=date_end,y=`Mean(R)`)) +
    geom_ribbon(aes(ymin=`Quantile.0.025(R)`,ymax=`Quantile.0.975(R)`),fill='grey75') +
    geom_line() + ggtitle('Brazil')
```

### Hubei Province


The incidence data probably need smoothing, and the time-dependent
model has unreasonable fluctuations.

```{r dohub,fig.height=7}
library(lubridate)
dates = lubridate::as_date(mdy(names(mar19df)[-c(1:4)]))
hubdat = as.numeric(get_series(province="Hubei", country="China", 
    dataset=sars2pack::mar19df))
names(hubdat) = dates
mGT <- generation.time("gamma", c(5.8, 0.95)) # from DOI 10.7326/M20-0504
mGT <- generation.time("gamma", c(3.96, 4.75)) # from DOI 10.7326/M20-0504
hubdat.filt = trim_leading_values(c(hubdat[1], diff(hubdat)))
est.EG <- estimate.R(epid=hubdat.filt, GT=mGT, 
    methods=c("EG", "TD"), begin=1L, end=as.integer(length(hubdat.filt)))
est.EG
par(mfrow=c(2,2), mar=c(5,3,2,2))
plot2(est.EG)
plotfit2(est.EG)
```

### Italy

For Italy, only the EG model seems to work, with the
Annals of Internal Medicine generation time model.  It
fits the data reasonably well, but the data seems to include
a reporting gap.

```{r doit,fig.height=7}
itdat = as.numeric(get_series(province="", country="Italy", 
    dataset=sars2pack::mar19df))
names(itdat) = dates
itdat.filt = trim_leading_values(c(itdat[1], diff(itdat)))
est.EG <- estimate.R(epid=itdat.filt, GT=mGT, 
    methods=c("EG"), begin=1L, end=as.integer(length(itdat.filt)))
est.EG
par(mfrow=c(2,1), mar=c(5,3,2,2))
plot2(est.EG, main="Italy")
plotfit2(est.EG, main="Italy")
```

### New York City

```{r warning=FALSE, message=FALSE}
nyt = nytimes_county_data() %>%
    dplyr::filter(county=='New York City' & subset=='confirmed') %>%
    dplyr::arrange(date)
nytdat = nyt$count
# do we need to chop zeros off? Seems like not.
nytdat.filt = c(nytdat[1], diff(nytdat))
est <- estimate.R(epid=nytdat.filt, GT=mGT, 
                  methods=c("EG","TD","ML"), begin=1L, end=as.integer(length(nytdat.filt)))
```

We can also use the package `r CRANpkg('EpiEstim')` to perform time-dependent $R_0$ calculations.

```{r warning=FALSE}
library(EpiEstim)
epiestim = EpiEstim::estimate_R(nytdat.filt, method = "parametric_si",
                                config = EpiEstim::make_config(list(
                                    mean_si = 3.96, std_si = 4.75)))
invisible(plot(epiestim))
```

## Italy from JHU dataset

This example uses data 

```{r, warning=FALSE}
jhu = jhu_data() %>%
    dplyr::filter(CountryRegion=='Italy' & is.na(ProvinceState) & subset=='confirmed') %>%
    dplyr::arrange(date)
jhucases = jhu$count
# do we need to chop zeros off? Seems like not.
jhucases.inc = c(jhucases[1], diff(jhucases))
jhucases.inc[jhucases.inc<0] = 0
epiestim = EpiEstim::estimate_R(jhucases.inc, method = "parametric_si",
                                config = EpiEstim::make_config(list(
                                                       mean_si = 3.96, std_si = 4.75)))
meanR = epiestim$R$`Mean(R)`
plot(meanR, type='l', ylim=c(0,5),main='R(t) for Italy', ylab='R', xlab='date')
abline(h=1, lty=2)
```


```{r include=FALSE}
fn = function(df, location_name,
              count_column='count',
              date_column='date',
              location_column='location') {
    df1 = df[df[[location_column]]==location_name,]
    df1.filt = c(df1[[count_column]][1],diff(df1[[count_column]]))
    df1.filt[df1.filt<0]=0
    df1 = data.frame(I = df1.filt, dates=df1$date)

    first_val = which(df1$I>10)[1]
    if(is.na(first_val)) {
        return(NULL)
    }
    df1 = df1[first_val:nrow(df1),]
    epiestim = EpiEstim::estimate_R(df1, method = "parametric_si",
                                    config = EpiEstim::make_config(list(
                                                           mean_si = 3.96, std_si = 4.75)))
    ret = tibble(epiestim$R)
    ret$location_name = location_name
    ret$date = as.Date(epiestim$dates[1:nrow(ret)])
    ret
}
```


## Simulated epidemic model

Following code conveyed by John Mallery, we have the following
approach for estimating $R_0$ using a single realization of
an epidemic simulation.


```{r dostraight}
library(sars2pack)
library(R0)
library(lubridate)
# Generating an epidemic with given parameters
mGT <- generation.time("gamma", c(3,1.5))
set.seed(5432)  # always initialize when simulating!
mEpid <- sim.epid(epid.nb=1, GT=mGT, epid.length=30, 
     family="poisson", R0=1.67, peak.value=500)
mEpid <- mEpid[,1]
# Running estimations
est <- estimate.R(epid=mEpid, GT=mGT, methods=c("EG","ML","TD"), begin=1, end=30)
```

We modified the plotting function in `r CRANpkg("R0")` which
was calling `dev.new` too often.  Use `plot2`.

```{r lksim,fig.height=7}
par(mfrow=c(2,2))
sars2pack::plot2(est)
```

The `plotfit2` function is also useful.  These fits
look identical but they are not.

```{r lksim2, fig.height=7}
par(mfrow=c(2,2))
sars2pack::plotfit2(est)
```

